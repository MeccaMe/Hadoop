package mp1;

import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class MyWordCount {
	
	public static class WordCountMapper extends Mapper<Object, Text, Text, IntWritable>{
		private Text word = new Text();
		private final static IntWritable one = new IntWritable(1);
		@Override
		protected void map(Object key, Text value, Mapper<Object, Text, Text, IntWritable>.Context context)
				throws IOException, InterruptedException {
			StringTokenizer stk = new StringTokenizer(value.toString());
			while(stk.hasMoreTokens()){
				word.set(stk.nextToken());
				context.write(word, one);
			}
		}	
	}
	
	public static class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable>{
		private  IntWritable result = new IntWritable();
		@Override
		protected void reduce(Text key, Iterable<IntWritable> values,
				Reducer<Text, IntWritable, Text, IntWritable>.Context context) throws IOException, InterruptedException {
			int sum = 0;
			for(IntWritable val : values){
				sum+=val.get();
			}
			result.set(sum);
			context.write(key, result);
		}		
	}

	public static void main(String[] args) {
		Configuration conf=new Configuration();
		conf.set("fs.default.name", "hdfs://localhost:9000");
//		conf.set("fs.hdfs.impl", org.apache.hadoop.hdfs.DistributedFileSystem.class.getName());
//		conf.set("fs.file.impl", org.apache.hadoop.fs.LocalFileSystem.class.getName());
		//得到一个Job 并设置名字  
        Job job;
		try {
			job = Job.getInstance(conf,"mywordcount");
        //设置Jar 使本程序在Hadoop中运行  
        job.setJarByClass(MyWordCount.class);  
        //设置Map处理类  
        job.setMapperClass(WordCountMapper.class);  
        //设置map的输出类型,因为不一致,所以要设置  
        job.setMapOutputKeyClass(Text.class);  
        job.setOutputValueClass(IntWritable.class);  
        //设置Reduce处理类  
        job.setReducerClass(WordCountReducer.class);  
        //设置输入和输出目录  
        FileInputFormat.addInputPath(job, new Path("/class1"));  
        FileOutputFormat.setOutputPath(job, new Path("/class1/output"));  
        //启动运行  
        System.exit(job.waitForCompletion(true) ? 0:1);  
		} catch (Exception e) {
			e.printStackTrace();
		}  
	}
}
